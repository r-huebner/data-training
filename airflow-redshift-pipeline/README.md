# Data Warehouse Pipeline with Airflow & Redshift

## ğŸ“š Project Overview

This project builds a data pipeline using **Apache Airflow**, **Amazon Redshift**, and **Amazon S3**. It loads raw JSON data from S3 into Redshift, transforms it into analytics-friendly star schema tables, and verifies data quality using custom checks.

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ final_project_dag.py             # Main DAG definition
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”œâ”€â”€ stage_redshift.py           # Stage data from S3 to Redshift
â”‚   â”‚   â”œâ”€â”€ load_dimension.py           # Load dimension tables
â”‚   â”‚   â”œâ”€â”€ load_fact.py                # Load fact table
â”‚   â”‚   â””â”€â”€ data_quality.py             # Run data quality checks
â”‚   â””â”€â”€ sql/
â”‚       â””â”€â”€ sql_statements.py           # SQL queries (e.g. COPY, CREATE, INSERT)
â””â”€â”€ README.md



---

## ğŸ—‚ï¸ Data Sources

- **S3**:
  - `s3://<your-bucket>/log-data/` â€“ Event logs
  - `s3://<your-bucket>/song-data/` â€“ Song metadata

---

## ğŸ§± Tables Overview

### Staging Tables
- `staging_events`
- `staging_songs`

### Fact Table
- `songplays`

### Dimension Tables
- `users`
- `songs`
- `artists`
- `time`

---

## âš™ï¸ Workflow Steps

1. **Create Tables** â€“ Executes `create_tables.sql` in Redshift
2. **Stage Data** â€“ Copies JSON files from S3 to `staging_*` tables
3. **Load Fact Table** â€“ Inserts into `songplays` with joins
4. **Load Dimensions** â€“ Populates dimension tables with distinct values
5. **Run Quality Checks** â€“ Verifies data integrity (e.g., non-empty, no NULL keys)

---

## ğŸ§© Requirements

- Python â‰¥ 3.7
- Apache Airflow â‰¥ 2.0
- AWS account with Redshift and S3 access
- Airflow Connections:
  - `aws_credentials` (type: Amazon Web Services)
  - `redshift` (type: Postgres)

---

## ğŸš€ Usage

> **Notice:** Ensure your Airflow connections (`aws_credentials`, `redshift`) are configured before running the DAG.

1. **Initialize the Airflow metadata database**  
   `airflow db init`

2. **Start the scheduler**  
   `airflow scheduler`

3. **Start the web server**  
   `airflow webserver`

4. **Trigger the DAG**  
   - In your browser, go to http://localhost:8080  
   - Switch the `final_project` DAG slider to â€œonâ€  
   - Click the â–¶ï¸ (run) button next to the latest scheduled run  


## ğŸ” Notes

- Use temporary AWS Access & Secret Keys for development only. Prefer IAM roles in production.  
- Redshift and S3 should be in the same AWS region.  
- `COPY` command uses `FORMAT AS JSON 'auto'` or a custom path.  
- Redshift user must have `COPY` permissions and access to referenced S3 bucket.  

## ğŸ“œ License

This project is part of the Udacity Data Engineering Nanodegree.  

